* 李航《统计学习方法》第9章
  * 三硬币例子。
  * 有点简略。
* Good. [【机器学习】EM——期望最大（非常详细）](https://zhuanlan.zhihu.com/p/78311644)
  * 写得不错。尤其是把Jensen不等式的过程解释了一下。一开始并没有意识到后面部分是期望，所以才不明白为什么叫EM算法。
  * L(θ) ≥ J(z, Q)。所以J(z, Q)是下界。可以不断最大化J(z, Q)来让L(θ)提高。EM算法的过程是先固定一个θ，调整Q使J(z, Q)上升至最大，即与L(θ)在当前θ处相等。然后固定Q，调整θ，使J(z, Q)达到最大值。重复这个过程。这里有两个问题：
    * 什么时候下界J(z, Q)与L(θ)相等？X=E(X)时，即随机变量是常量时，Jensen不等式取等。可以推出Q(z) = p(z|xi;θ)。
    * 为什么一定会收敛？
  * 通常引入隐变量后会有两个参数，例如J(z, Q)里包含两个参数θ和Q。EM算法的E步会固定其中的第一个参数，然后使用MLE（极大似然估计）计算第二个变量值；然后M步通过固定第二个变量，再使用MLE估计第一个变量值。依次迭代，直到收敛。
* Good. [人人都懂EM算法](https://zhuanlan.zhihu.com/p/36331115)
  * 极大似然估计
  * K-Means算法也类似与EM算法。
  * 证明了收敛性
  * 使用了两个硬币作为例子。